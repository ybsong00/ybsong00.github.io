<!DOCTYPE html>
<html>
<head>
<title>Yibing Song</title>

<link rel="stylesheet" type="text/css" href="stylesheet.css">
<style>
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;  
  background: #fdfdfd;
}
</style>

</head>

<body>
		
	<table align="center" cellspacing="10">
	<tr>
	<td align="center"><img border=0 height=200 width=200 src="ybsong.jpg"></td>
	<td align="center">
			<td align="center"><h2>Yibing Song</h2>
			<p><font size=+1>&#23435;&#22869;&#20853;</font></p>
			<p><font size=+1>DAMO Academy</font><br>
			<font size=+1>Alibaba Group</font></p>						
			<p>Email: yibingsong.cv at gmail dot com<br></p>
			</td>			
	</td>		
	</tr>
	</table>
	
	<h2>Biography</h2>
	<hr/>
	<p><font size="3">I am a research scientist in DAMO Academy. Previously, I was a faculty member in Fudan University, and a senior researcher in Tencent AI Lab. I got my PhD/Mphil degrees from City University of Hong Kong during which I visited Adobe Research and UC Merced, and got my bachelor degree from University of Science and Technology of China. Currently, my research interest resides in multi-modality AI, from both model-centric and data-centric perspectives, with applications centered around computer vision. I have been elected among Top 2% Scientists worldwide by Stanford University.</font></p>

<p><font color="#A52A2A"><i>News: I am building up a multi-modality AI team in DAMO Academy, i.e., seeking talents in research scientist/engineer/intern positions at all levels. Besides, we have fundings for university collaborations. All these two openings are related to multi-modality foundation models. If you are interested in either, contact me directly.</i></font></p>

	<h2>Service Highlights</h2>
	<hr>
	<p><font size="3">Area Chairs / Meta Reviewers: CVPR (2024,2023), ICCV 2023, NeurIPS (2023,2022), ICML (2024,2023), ICLR (2024,2023,2022).</font></p>		
	<p><font size="3">Outstanding / Top Reviewers: CVPR (2020,2019,2018), ECCV 2022, NeurIPS 2019.</font></p>
	<br>	

	<h2>Representative Publications &nbsp
	<a href="pubs.html">[More]</a>
	<a href="http://scholar.google.com/citations?user=oRhJHmIAAAAJ">[Citations]</a></h2>
	<hr>
	<table cellspacing="10">
	<tbody>
				
		<tr>
			<td><center><img width="300" src="teaser_figs/InstructDET.png"></center></td>
			<td>
			<font size="3">
				  <a href="https://arxiv.org/abs/2310.05136">
				  <papertitle>
					InstructDET: Diversifying Referring Object Detection with Generalized Instructions
				  </papertitle>		
			          </a>
			<br>
			<i>Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, and <b>Yibing Song</b>,</i>
			<br>
			International Conference on Learning Representations (<b>ICLR</b>) 2024
			<br>
			<a href="https://arxiv.org/abs/2310.05136">Paper</a> /    
			<a href="https://github.com/jyFengGoGo/InstructDet">Project</a>	
			</font>
			</td>			
		</tr>	
			
		<tr>
			<td><center><img width="300" src="teaser_figs/iccv23_diffusiondet.png"></center></td>
			<td bgcolor="#F5F5F5">
			<font size="3">
				<a href="https://arxiv.org/abs/2211.09788">
				  <papertitle>
					DiffusionDet: Diffusion Model for Object Detection
				  </papertitle>		
				</a>
			<br>
			<i>Shoufa Chen, Peize Sun, <b>Yibing Song</b>, and Ping Luo,</i>
			<br>
			IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2023 (<b><font color="red">Best Paper Nominee</font></b>)
			<br>
			<a href="https://arxiv.org/abs/2211.09788">Paper</a> /               				
            		<a href="https://github.com/ShoufaChen/DiffusionDet">Project</a>  
			</font>					
			</td>			
		</tr>	


		<tr>
			<td><center><img width="300" src="teaser_figs/nips22_videomae.png"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2203.12602">
                  		<papertitle>			  
                    		VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training
                  		</papertitle>
                		</a>					
			<br>
			<i>Zhan Tong, <b>Yibing Song</b>, Jue Wang, and Limin Wang,</i>
			<br>
			Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2022 (<b>Spotlight</b>)
			<br>
			<a href="https://arxiv.org/abs/2203.12602">Paper</a> /               				
            		<a href="https://github.com/MCG-NJU/VideoMAE">Project</a> /
			<a href="https://huggingface.co/docs/transformers/main/en/model_doc/videomae">Hugging Face Repo</a>
			<br>
			<a href="https://www.paperdigest.org/2023/01/most-influential-nips-papers-2023-01/">Ranked 8th in most influential NIPS 2022 papers</a> / 
			<a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">Ranked 39th in most cited 2022 AI papers</a>
			</font>
			</td>			
		</tr>			
		
		
	</tbody>
	</table>
	
		
</body>

</html>


