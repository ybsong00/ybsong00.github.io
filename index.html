<!DOCTYPE html>
<html>
<head>
<title>Yibing Song</title>

<link rel="stylesheet" type="text/css" href="stylesheet.css">
<style>
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;  
  background: #fdfdfd;
}
</style>

</head>

<body>
		
	<table align="center" cellspacing="10">
	<tr>
	<td align="center"><img border=0 height=200 width=200 src="ybsong.jpg"></td>
	<td align="center">
			<td align="center"><h2>Yibing Song</h2>
			<p><font size=+1>&#23435;&#22869;&#20853;</font></p>
			<p><font size=+1>DAMO Academy</font><br>
			<font size=+1>Alibaba Group</font></p>						
			<p>Email: yibingsong.cv at gmail dot com<br></p>
			</td>			
	</td>		
	</tr>
	</table>
	
	<h2>Biography</h2>
	<hr/>
	<p><font size="3">I am a staff scientist / research manager at Alibaba DAMO Academy. Previously, I was a faculty member in Fudan University, and a senior researcher in Tencent AI Lab. I got my PhD/MPhil degrees from City University of Hong Kong during which I visited Adobe Research and UC Merced, and got my bachelor degree from University of Science and Technology of China. My research resides in computer vision and machine learning, with 50+ premier papers (i.e., CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, PAMI, IJCV) published and nearly 10k citations gathered. Currently, I am interested in multi-modality AI, from model-centric, data-centric, and human-centric perspectives, with applications centered around computer vision. I am an IEEE senior member, and have been elected among Top 2% Scientists worldwide by Stanford University.</font></p>
	<br>

	<h2>Professional Activities</h2>
	<hr>
	<p><font size="3"><table>
		<tr>
			<td>Area Chairs / Meta Reviewers: </td>
			<td>CVPR (2024, 2023), ICCV 2023</td>
		</tr>
		<tr>
			<td></td>
			<td>NeurIPS (2024, 2023, 2022), ICML (2024, 2023), ICLR (2025, 2024, 2023, 2022)</td>
		</tr>
	</table></font></p>		
	<p><font size="3"><table>
		<tr>
			<td>Outstanding / Top Reviewers: </td>
			<td>CVPR (2020, 2019, 2018), ECCV 2022, NeurIPS 2019</td>
		</tr>
	</table></font></p>
	<br>	

	<h2>Some Papers</h2>
	<hr>
	<table cellspacing="10">
	<tbody>
		<tr>
			<td><center><img width="300" src="teaser_figs/iccv23_diffusiondet.png"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2211.09788">
				  <papertitle>
					DiffusionDet: Diffusion Model for Object Detection
				  </papertitle>		
				</a>
			<br>
			<i>Shoufa Chen, Peize Sun, <b>Yibing Song</b>, and Ping Luo,</i>
			<br>
			IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2023 (<b><font color="red">Best Paper Nominee</font></b>)
			<br>
			<a href="https://arxiv.org/abs/2211.09788">Paper</a> /               				
            		<a href="https://github.com/ShoufaChen/DiffusionDet">Project</a>  
			</font>					
			</td>			
		</tr>	


		<tr>
			<td><center><img width="300" src="teaser_figs/nips22_videomae.png"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2203.12602">
                  		<papertitle>			  
                    		VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training
                  		</papertitle>
                		</a>					
			<br>
			<i>Zhan Tong, <b>Yibing Song</b>, Jue Wang, and Limin Wang,</i>
			<br>
			Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2022 (<b>Spotlight</b>)
			<br>
			<a href="https://arxiv.org/abs/2203.12602">Paper</a> /               				
            		<a href="https://github.com/MCG-NJU/VideoMAE">Project</a> /
			<a href="https://huggingface.co/docs/transformers/main/en/model_doc/videomae">Hugging Face Repo</a>
			<br>
			<a href="https://www.paperdigest.org/2023/01/most-influential-nips-papers-2023-01/">Ranked 8th in most influential NIPS 2022 papers</a> / 
			<a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">Ranked 39th in most cited 2022 AI papers</a>
			</font>
			</td>			
		</tr>			
		
		
	</tbody>
	</table>
	
		
</body>

</html>


