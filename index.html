<!DOCTYPE html>
<html>
<head>
<title>Yibing Song</title>

<link rel="stylesheet" type="text/css" href="stylesheet.css">
<style>
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;  
  background: #fdfdfd;
}
</style>

</head>

<body>
		
	<table align="center" cellspacing="10">
	<tr>
	<td align="center"><img border=0 height=204 width=200 src="ybsong.png"></td>
	<td align="center">
			<td align="center"><h2>Yibing Song</h2>
			<p><font size=+1>&#23435;&#22869;&#20853;</font></p>
			<p><font size=+1>Computer Vision Researcher<br/>Tencent AI Lab</font></p>						
			<p>Email: yibingsong.cv at gmail dot com<br></p>
			</td>			
	</td>		
	</tr>
	</table>
	
	<h2>Biography</h2>
	<hr/>
	<p><font size="3">Yibing Song works for Tencent. He has obtained PhD/Mphil degrees from City University of Hong Kong in 2018/2014 and a bachelor degree from University of Science and Technology of China in 2011. During graduate study, he has interned in Adobe Research, San Jose and visited UC Merced. 
	Yibing Song works on the fundamental visual recognitions that develop pioneer neural architectures with representation learning. Meanwhile, he is interested in emerging visual generations with single or multiple modalities.</font></p>
		
	<br>
	
	<h2>Service Highlights</h2>
	<hr>
	<p><font size="3">Area Chairs / Meta Reviewers: CVPR 2023, ICCV 2023, NeurIPS (2023, 2022), ICML 2023, ICLR (2023, 2022).</font></p>		
	<p><font size="3">Outstanding / Top Reviewers: CVPR (2020, 2019, 2018), ECCV 2022, NeurIPS 2019.</font></p>
	<br>	
	
	<h2>3 Representative Publications &nbsp
	<a href="pubs.html">[More]</a>
	<a href="http://scholar.google.com/citations?user=oRhJHmIAAAAJ">[Citations]</a></h2>
	<hr>
	<table cellspacing="10">
	<tbody>
	
	
		<tr>
			<td><center><img width="300" src="teaser_figs/nips22_videomae.png"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2203.12602">
                  		<papertitle>			  
                    		VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training
                  		</papertitle>
                		</a>					
			<br>
			<i>Zhan Tong, <b>Yibing Song</b>, Jue Wang, and Limin Wang,</i>
			<br>
			Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2022 (<b>Spotlight</b>)
			<br>
			<a href="https://arxiv.org/abs/2203.12602">Paper</a> /               				
            		<a href="https://github.com/MCG-NJU/VideoMAE">Project</a> /
			<a href="https://huggingface.co/docs/transformers/main/en/model_doc/videomae">Hugging Face Repo</a>
			<br>
			<a href="https://www.paperdigest.org/2023/01/most-influential-nips-papers-2023-01/">Ranked 8th in most influential NIPS 2022 papers</a> / 
			<a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">Ranked 39th in most cited 2022 AI papers</a>
			</font>
			</td>			
		</tr>

		
		<tr>
			<td><center><img width="300" src="teaser_figs/nips22_ffclip.png"></center></td>
			<td bgcolor="#F5F5F5">
			<font size="3">
				  <a href="http://arxiv.org/abs/2210.07883">
				  <papertitle>
					One Model to Edit Them All: Free-Form Text-Driven Image Manipulation with Semantic Modulations
				  </papertitle>		
			          </a>
			<br>
			<i>Yiming Zhu, Hongyu Liu, <b>Yibing Song</b>, Ziyang Yuan, Xintong Han, Chun Yuan, Qifeng Chen, and Jue Wang,</i>
			<br>
			Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2022 (<b>Spotlight</b>)
			<br>
			<a href="http://arxiv.org/abs/2210.07883">Paper</a> /    
			<a href="https://github.com/KumapowerLIU/FFCLIP">Project</a> /
			<a href="https://mp.weixin.qq.com/s/f0QiRL4bEplBw_9THKzxbQ">Media Report</a>	
			</font>
			</td>			
		</tr>
		
		
		<tr>
			<td><center><img width="300" src="teaser_figs/nips22_adaptformer.png"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2205.13535">
				  <papertitle>
					AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition
				  </papertitle>		
				</a>
			<br>
			<i>Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, <b>Yibing Song</b>, Jue Wang, and Ping Luo,</i>
			<br>
			Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2022
			<br>
			<a href="https://arxiv.org/abs/2205.13535">Paper</a> /               				
            		<a href="https://github.com/ShoufaChen/AdaptFormer">Project</a>                            
			</font>					
			</td>			
		</tr>							
		
		
	</tbody>
	</table>
	
		
</body>

</html>


