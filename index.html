<!DOCTYPE html>
<html>
<head>
<title>Yibing Song</title>

<link rel="stylesheet" type="text/css" href="stylesheet.css">
<style>
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;  
  background: #fdfdfd;
}
</style>

</head>

<body>
		
	<table align="center" cellspacing="10">
	<tr>
	<td align="center"><img border=0 height=200 width=200 src="ybsong.jpg"></td>
	<td align="center">
			<td align="center"><h2>Yibing Song</h2>
			<p><font size=+1>&#23435;&#22869;&#20853;</font></p>
			<p><font size=+1>Deputy Chief Engineer (Algorithm)</font><br>
			<font size=+1>BYD Group</font></p>						
			<p>Email: yibingsong.cv at gmail dot com<br></p>
			</td>			
	</td>		
	</tr>
	</table>
	
	<h2>Biography</h2>
	<hr/>
	<p><font size="3">I oversee the AI system design in BYD electric vehicles. Previously, I held positions in Academia (i.e., Fudan University as a faculty member) and Industry (i.e., Alibaba DAMO Academy, and Tencent AI Lab as a research scientist). I got my PhD/MPhil degrees from City University of Hong Kong during which I visited Adobe Research and UC Merced, and got my bachelor degree from University of Science and Technology of China. My expertise resides in computer vision and machine learning, with 60+ premier papers (i.e., CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, PAMI, IJCV) published and 12k+ citations gathered. Specifically, I am experienced in multi-modal AI, from model-centric, data-centric, and human-centric perspectives, with applications centered around computer vision. I am an IEEE senior member, and have been elected among the Top 2% Scientists worldwide by Stanford University.</font></p>
	<br>
	
	<h2>Professional Activities</h2>
	<hr>
	<p><table>
		<tr>
			<td><font size="3">Senior Area Chairs: </font></td>
			<td><font size="3">ICLR 2026</font></td>
		</tr>
		<tr>
			<td><font size="3">Area Chairs / Meta Reviewers: </font></td>
			<td><font size="3">CVPR (2023-2025), ICCV (2023-2025), NeurIPS (2022-2025), ICML (2023-2025), ICLR (2022-2025)</font></td>
		</tr>
	</table></p>		
	<p><table>
		<tr>
			<td><font size="3">Outstanding / Top Reviewers: </font></td>
			<td><font size="3">CVPR (2018-2020), ECCV 2022, NeurIPS 2019</font></td>
		</tr>
	</table></p>
	<br>	

	<h2>Shortlisted Publications &nbsp
		<a href="pubs.html">[More]</a>
		<a href="http://scholar.google.com/citations?user=oRhJHmIAAAAJ">[Citations]</a>
	</h2> 
	<hr>
	<table cellspacing="10">
	<body>
		<tr>
			<td><center><img width="300" src="teaser_figs/cot-diff.png"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2507.04451">
                  		<papertitle>			  
                    		CoT-lized Diffusion: Let's Reinforce T2I Generation Step-by-Step
                  		</papertitle>
                		</a>					
			<br>
			<i>Zheyuan Liu, Munan Ning, Qihui Zhang, Shuo Yang, Zhongrui Wang, Yiwei Yang, Xianzhe Xu, <b>Yibing Song</b>, Weihua Chen, Fan Wang, and Li Yuan</i>
			<br>
			Arxiv Preprint 2025
			<br>
			<a href="https://arxiv.org/abs/2507.04451">Paper</a> /               				
            		<a>Project</a> 			
			</font>
			</td>			
		</tr>		


		
		<tr>
			<td><center><img width="300" src="teaser_figs/llava_o1.jpg"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2411.10440">
                  		<papertitle>			  
                    		LLaVA-CoT: Let Vision Language Models Reason Step-by-Step
                  		</papertitle>
                		</a>					
			<br>
			<i>Guowei Xu, Peng Jin, Li Hao, <b>Yibing Song</b>, Lichao Sun, and Li Yuan,</i>
			<br>
			IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2025
			<br>
			<a href="https://arxiv.org/abs/2411.10440">Paper</a> /               				
            		<a href="https://github.com/PKU-YuanGroup/LLaVA-o1">Project</a> 			
			</font>
			</td>			
		</tr>

		<tr>
			<td><center><img width="300" src="teaser_figs/iclr25_rlod.jpg"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2503.23508">
                  		<papertitle>			  
                    		Re-Aligning Language to Visual Objects with an Agentic Workflow
                  		</papertitle>
                		</a>					
			<br>
			<i>Yuming Chen, Jiangyan Feng, Haodong Zhang, Lijun Gong, Feng Zhu, Rui Zhao, Qibin Hou, Ming-Ming Cheng, and <b>Yibing Song</b>,</i>
			<br>
			International Conference on Learning Representations (<b>ICLR</b>) 2025
			<br>
			<a href="https://arxiv.org/abs/2503.23508">Paper</a> /               				
            		<a href="https://github.com/FishAndWasabi/RealLOD">Project</a> 			
			</font>
			</td>			
		</tr>


		
		<tr>
			<td><center><img width="300" src="teaser_figs/iccv23_diffusiondet.png"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2211.09788">
				  <papertitle>
					DiffusionDET: Diffusion Model for Object Detection
				  </papertitle>		
				</a>
			<br>
			<i>Shoufa Chen, Peize Sun, <b>Yibing Song</b>, and Ping Luo,</i>
			<br>
			IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2023 (<b><font color="red">Best Paper Nominee</font></b>)
			<br>
			<a href="https://arxiv.org/abs/2211.09788">Paper</a> /               				
            		<a href="https://github.com/ShoufaChen/DiffusionDet">Project</a>  
			</font>					
			</td>			
		</tr>	


		<tr>
			<td><center><img width="300" src="teaser_figs/nips22_videomae.png"></center></td>
			<td>
			<font size="3">
				<a href="https://arxiv.org/abs/2203.12602">
                  		<papertitle>			  
                    		VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training
                  		</papertitle>
                		</a>					
			<br>
			<i>Zhan Tong, <b>Yibing Song</b>, Jue Wang, and Limin Wang,</i>
			<br>
			Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2022 (<b>Spotlight</b>)
			<br>
			<a href="https://arxiv.org/abs/2203.12602">Paper</a> /               				
            		<a href="https://github.com/MCG-NJU/VideoMAE">Project</a> /
			<a href="https://huggingface.co/docs/transformers/main/en/model_doc/videomae">Hugging Face Repo</a>
			<br>
			<a href="https://www.paperdigest.org/2023/01/most-influential-nips-papers-2023-01/">Ranked 8th in most influential NIPS 2022 papers</a> / 
			<a href="https://www.zeta-alpha.com/post/must-read-the-100-most-cited-ai-papers-in-2022">Ranked 39th in most cited 2022 AI papers</a>
			</font>
			</td>			
		</tr>			
		
		
	</tbody>
	</table>
	
		
</body>

</html>


